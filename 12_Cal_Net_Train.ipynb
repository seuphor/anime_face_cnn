{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import save \n",
    "import numpy as np\n",
    "from models._12_net import calib_12Net\n",
    "from utils.utilities import batch_selection\n",
    "import sys\n",
    "from utils.params import *\n",
    "import time\n",
    "from utils import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_correct_percentage(pred, truth, top_numb=3):\n",
    "    assert(truth.shape[0] == pred.shape[0])\n",
    "    top_score = [arr.argsort()[-top_numb:][::-1] for arr in pred]\n",
    "    score_list = [1 if truth[i] in top_score[i] else 0 for i in range(truth.shape[0])]\n",
    "    percentage = np.sum(score_list) / truth.shape[0]\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 2000\n",
    "lr = [1e-3, 7e-4, 4e-4]\n",
    "bias_init = [0.0]\n",
    "reg = [0.0, 1e-4]\n",
    "_12_calib_batch = batch_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from D:\\programing project\\python project\\trying stuffpickle/\n"
     ]
    }
   ],
   "source": [
    "calibrate_db_12 = save.loader('calibrate_db_12_long.txt')\n",
    "calibrate_db_12 = calibrate_db_12[:int(len(calibrate_db_12) / batch_size) * batch_size]\n",
    "inputs_calib_12 = np.zeros((len(calibrate_db_12), 12, 12, 3))\n",
    "targets_calib_12 = np.zeros((len(calibrate_db_12), 45), dtype=np.int32)\n",
    "\n",
    "for i in range(len(calibrate_db_12)):\n",
    "    inputs_calib_12[i,:] = calibrate_db_12[i][0]\n",
    "    targets_calib_12[i,calibrate_db_12[i][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = np.zeros((batch_size, 12, 12, 3))\n",
    "targets = np.zeros((batch_size, 45), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g=tf.Graph()\n",
    "with g.as_default():\n",
    "    input_12_node = tf.placeholder(tf.float32, [None, 12, 12, 3])\n",
    "    target_12_node = tf.placeholder(tf.float32, [None, 45])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    _calib_12 = calib_12Net(input_12_node, target_12_node, keep_prob, 0.0, 0.0)\n",
    "\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "  allow_soft_placement=True, log_device_placement=True), graph=g)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from D:\\programing project\\python project\\trying stuffpickle/\n"
     ]
    }
   ],
   "source": [
    "result_list = save.loader('calibrate_db_12_test.txt')\n",
    "valid_input = np.concatenate([result_list[i][0][np.newaxis] for i in range(len(result_list))])\n",
    "valid_target = np.asarray([result_list[i][1] for i in range(len(result_list))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Bias_init:0.0   Learning_rate:0.001   Regularization:0.0**\n",
      "Epoch:834    Iters:65052/156000    Loss:1.15456    Cur_Accuracy:49.55%/77.33%    Max_Acc/Epo:52.88%/78.55%/235    Unimprove_Count:119\n",
      "Trained Time:9.333 mins\n",
      "\n",
      "**Bias_init:0.0   Learning_rate:0.0007   Regularization:0.0**\n",
      "Epoch:789    Iters:61542/156000    Loss:0.69467    Cur_Accuracy:51.11%/77.55%    Max_Acc/Epo:56.22%/82.44%/190    Unimprove_Count:119\n",
      "Trained Time:9.140 mins\n",
      "\n",
      "**Bias_init:0.0   Learning_rate:0.0004   Regularization:0.0**\n",
      "Epoch:714    Iters:55692/156000    Loss:0.491528    Cur_Accuracy:55.11%/78.88%    Max_Acc/Epo:60.0%/82.11%/115    Unimprove_Count:119\n",
      "Trained Time:7.898 mins\n",
      "\n",
      "**Bias_init:0.0   Learning_rate:0.001   Regularization:0.0001**\n",
      "Epoch:1129    Iters:88062/156000    Loss:1.51238    Cur_Accuracy:49.88%/78.0%    Max_Acc/Epo:48.55%/79.33%/530    Unimprove_Count:119\n",
      "Trained Time:12.96 mins\n",
      "\n",
      "**Bias_init:0.0   Learning_rate:0.0007   Regularization:0.0001**\n",
      "Epoch:954    Iters:74412/156000    Loss:1.11557    Cur_Accuracy:52.22%/77.55%    Max_Acc/Epo:52.66%/79.88%/355    Unimprove_Count:119\n",
      "Trained Time:10.82 mins\n",
      "\n",
      "**Bias_init:0.0   Learning_rate:0.0004   Regularization:0.0001**\n",
      "Epoch:819    Iters:63882/156000    Loss:0.708017    Cur_Accuracy:58.22%/81.66%    Max_Acc/Epo:59.55%/82.33%/220    Unimprove_Count:119\n",
      "Trained Time:9.197 mins\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in reg:\n",
    "    for b in bias_init:\n",
    "        for l in lr:\n",
    "            record_l = l\n",
    "            print('**Bias_init:{}   Learning_rate:{}   Regularization:{}**'.format(b,l,r))\n",
    "\n",
    "            max_i = 0\n",
    "            correct_pnt_top3 = 0\n",
    "            correct_pnt_top1 = 0\n",
    "            correct_pnt_top3_max = 0\n",
    "            correct_pnt_top1_max = 0\n",
    "            bad_cnt = 0\n",
    "\n",
    "            start = time.time()\n",
    "            g=tf.Graph()\n",
    "            with g.as_default():\n",
    "                input_12_node = tf.placeholder(tf.float32, [None, 12, 12, 3])\n",
    "                target_12_node = tf.placeholder(tf.float32, [None, 45])\n",
    "                keep_prob = tf.placeholder(tf.float32)\n",
    "                _calib_12 = calib_12Net(input_12_node, target_12_node, keep_prob, l, b, r)\n",
    "\n",
    "            sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "              allow_soft_placement=True, log_device_placement=True), graph=g)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #train_writer = tf.summary.FileWriter('./logs/train/Calib_12/bias_init={},lr={}'.format(b,l))\n",
    "\n",
    "            for i in range(epochs):\n",
    "\n",
    "                if (i+1) % 5 == 0:\n",
    "                    pred_target = _calib_12.prediction.eval(feed_dict={input_12_node:valid_input,keep_prob:1.0})\n",
    "                    correct_pnt_top3 = valid_correct_percentage(pred_target, valid_target, 3)\n",
    "                    correct_pnt_top1 = valid_correct_percentage(pred_target, valid_target, 1)\n",
    "\n",
    "                    if correct_pnt_top3 >= correct_pnt_top3_max:\n",
    "                        max_i = i + 1\n",
    "                        correct_pnt_top3_max = correct_pnt_top3\n",
    "                        correct_pnt_top1_max = correct_pnt_top1\n",
    "                        bad_cnt = 0\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, p_model_dir + 'calib_12/max/_Net12_b{}_lr{}_reg{}_drop{}.ckpt'.format(b,record_l,r,0.5))\n",
    "\n",
    "                    else:\n",
    "                        bad_cnt += 1\n",
    "\n",
    "                if bad_cnt % 5 == 0:\n",
    "                    l = l * .98\n",
    "                    \n",
    "                if bad_cnt >= 120:\n",
    "                    break\n",
    "\n",
    "                for batch_i in range(int(len(calibrate_db_12) / batch_size)):\n",
    "                    iters = i*int(len(calibrate_db_12) / batch_size) + batch_i\n",
    "                    inputs, targets = _12_calib_batch.next_batch(inputs_calib_12, targets_calib_12, batch_size)\n",
    "                    feed_train = {\n",
    "                        input_12_node:inputs,\n",
    "                        target_12_node:targets,\n",
    "                        keep_prob:.5\n",
    "                    }\n",
    "                    feed_loss = {\n",
    "                        input_12_node:inputs,\n",
    "                        target_12_node:targets,\n",
    "                        keep_prob:1.0\n",
    "\n",
    "                    }\n",
    "\n",
    "                    cost = _calib_12.loss.eval(feed_loss)\n",
    "                    sys.stdout.write('\\rEpoch:' + str(i+1) + \\\n",
    "                                     '    Iters:' + str(iters+1) + '/' + str(int(len(calibrate_db_12) / batch_size)*epochs) + \\\n",
    "                                     '    Loss:' + str(cost)[:8] + \\\n",
    "                                     '    Cur_Accuracy:' + str(correct_pnt_top1*100)[:5] + '%/' + str(correct_pnt_top3*100)[:5] + '%' + \\\n",
    "                                     '    Max_Acc/Epo:' + str(correct_pnt_top1_max*100)[:5] + '%/' + str(correct_pnt_top3_max*100)[:5] + \\\n",
    "                                                          '%/' + str(max_i) + \\\n",
    "                                     '    Unimprove_Count:' + str(bad_cnt))\n",
    "                    #if (iters + 1) % 10 == 0 or iters == 0:\n",
    "                        #summary = _calib_12.merged.eval(feed_loss)\n",
    "                        #train_writer.add_summary(summary, iters)\n",
    "\n",
    "                    _calib_12.train_step.run(feed_train)\n",
    "            end = time.time()\n",
    "            print('\\nTrained Time:{} mins\\n'.format(str((end-start)/60.0)[:5]))\n",
    "            #saver = tf.train.Saver()\n",
    "            #saver.save(sess, p_model_dir + 'calib_12/_Net12_b{}_lr{}drop{}.ckpt'.format(b,l,0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Bias_init:0.0   Learning_rate:0.01**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.348112    Trained Time:1.873 mins\n",
      "\n",
      "**Bias_init:0.0   Learning_rate:0.005**\n",
      "Epoch:600    Iters:11400/11400    Loss:1.36653    Trained Time:1.885 mins\n",
      "\n",
      "**Bias_init:0.0   Learning_rate:0.001**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.01272    Trained Time:1.883 mins\n",
      "\n",
      "**Bias_init:0.01   Learning_rate:0.01**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.044465    Trained Time:1.876 mins\n",
      "\n",
      "**Bias_init:0.01   Learning_rate:0.005**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.991462    Trained Time:1.884 mins\n",
      "\n",
      "**Bias_init:0.01   Learning_rate:0.001**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.079620    Trained Time:1.889 mins\n",
      "\n",
      "**Bias_init:0.1   Learning_rate:0.01**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.409649    Trained Time:1.874 mins\n",
      "\n",
      "**Bias_init:0.1   Learning_rate:0.005**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.130105    Trained Time:1.883 mins\n",
      "\n",
      "**Bias_init:0.1   Learning_rate:0.001**\n",
      "Epoch:600    Iters:11400/11400    Loss:0.018728    Trained Time:1.865 mins\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in bias_init:\n",
    "    for l in lr:\n",
    "        print('**Bias_init:{}   Learning_rate:{}**'.format(b,l))\n",
    "        start = time.time()\n",
    "        g=tf.Graph()\n",
    "        with g.as_default():\n",
    "            input_12_node = tf.placeholder(tf.float32, [None, 12, 12, 3])\n",
    "            target_12_node = tf.placeholder(tf.float32, [None, 45])\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            _calib_12 = calib_12Net(input_12_node, target_12_node, keep_prob, b, l)\n",
    "        \n",
    "        sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "          allow_soft_placement=True, log_device_placement=True), graph=g)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter('./logs/train/Calib_12/bias_init={},lr={}'.format(b,l))\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for batch_i in range(int(len(calibrate_db_12) / batch_size)):\n",
    "                iters = i*int(len(calibrate_db_12) / batch_size) + batch_i\n",
    "                inputs, targets = _12_calib_batch.next_batch(inputs_calib_12, targets_calib_12, batch_size)\n",
    "                feed_train = {\n",
    "                    input_12_node:inputs,\n",
    "                    target_12_node:targets,\n",
    "                    keep_prob:0.8\n",
    "                }\n",
    "                feed_loss = {\n",
    "                    input_12_node:inputs,\n",
    "                    target_12_node:targets,\n",
    "                    keep_prob:1.0\n",
    "                    \n",
    "                }\n",
    "                \n",
    "                cost = _calib_12.loss.eval(feed_loss)\n",
    "                sys.stdout.write('\\rEpoch:' + str(i+1) + \\\n",
    "                                 '    Iters:' + str(iters+1) + '/' + str(int(len(calibrate_db_12) / batch_size)*epochs) + \\\n",
    "                                 '    Loss:' + str(cost)[:8])\n",
    "                if (iters + 1) % 10 == 0 or iters == 0:\n",
    "                    summary = _calib_12.merged.eval(feed_loss)\n",
    "                    train_writer.add_summary(summary, iters)\n",
    "                \n",
    "                _calib_12.train_step.run(feed_train)\n",
    "        end = time.time()\n",
    "        print('    Trained Time:{} mins\\n'.format(str((end-start)/60.0)[:5]))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, p_model_dir + 'calib_12/_Net12_b{}_lr{}drop{}.ckpt'.format(b,l,0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:0 Loss:3.80697\n",
      "Epochs:9 Loss:2.54898\n",
      "Epochs:19 Loss:2.36249\n",
      "Epochs:29 Loss:2.33224\n",
      "Epochs:39 Loss:2.31482\n",
      "Epochs:49 Loss:2.30627\n",
      "Epochs:59 Loss:2.30354\n",
      "Epochs:69 Loss:2.30283\n",
      "Epochs:79 Loss:2.3027\n",
      "Epochs:89 Loss:2.30267\n",
      "Epochs:99 Loss:2.30263\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    summary = _calib_12.merged.eval(feed)\n",
    "    train_writer.add_summary(summary, i)\n",
    "    if (i + 1) % 10 == 0 or i == 0:\n",
    "        cost = _calib_12.loss.eval(feed)\n",
    "        print('Epochs:{}'.format(str(i)), 'Loss:{}'.format(str(cost)))\n",
    "    _calib_12.train_step.run(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calibrate_db_12_inputs = np.concatenate([calibrate_db_12[i][0][np.newaxis] for i in range(len(calibrate_db_12))])\n",
    "calibrate_db_12_targets = np.array([calibrate_db_12[i][1] for i in range(len(calibrate_db_12))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_init = 0.0\n",
    "lr = 5e-2\n",
    "w_std = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, scale, name=None):\n",
    "    init = tf.truncated_normal(shape=shape, stddev=w_std) / scale\n",
    "    return tf.Variable(init, name=name)\n",
    "\n",
    "def weight_variable_refine(shape, scale, name=None):\n",
    "    init = tf.random_normal(shape=shape) / scale\n",
    "    return tf.Variable(init, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    init = tf.constant(value=b_init, shape=shape)\n",
    "    return tf.Variable(init, name=name)\n",
    "\n",
    "def conv2d(x, W, stride, pad = 'SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,stride,stride,1], padding=pad)\n",
    "\n",
    "def max_pool(x, kersize, stride, pad = 'SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=[1,kersize,kersize,1], strides=[1,stride,stride,1], padding=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_12_node = tf.placeholder(tf.float32, [None, 12, 12, 3])\n",
    "target_12_node = tf.placeholder(tf.float32, [None, 45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conv1\n",
    "w_conv1 = weight_variable([3,3,3,16], tf.sqrt(3*3*3.0/2), 'w1')\n",
    "b_conv1 = bias_variable([16], 'b1')\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(input_12_node, w_conv1, 1) + b_conv1)\n",
    "h_pool1 = max_pool(h_conv1, 3, 2)\n",
    "\n",
    "# fc1\n",
    "w_fc1 = weight_variable([6*6*16,128], tf.sqrt(6*6*16.0/2), 'w2')\n",
    "b_fc1 = bias_variable([128], 'b2')\n",
    "h_pool1_flat = tf.reshape(h_pool1, [-1, 6*6*16])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# fc_conv2\n",
    "w_fc2 = weight_variable([128,45], tf.sqrt(128.0/2), 'w3')\n",
    "b_fc2 = bias_variable([45], 'b3')\n",
    "h_fc2 = tf.matmul(h_fc1, w_fc2) + b_fc2\n",
    "\n",
    "# softmax prediction\n",
    "prediction = tf.nn.softmax(h_fc2)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h_fc2, labels=target_12_node))\n",
    "\n",
    "# optimizer\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = np.zeros((batch_size, 12, 12, 3))\n",
    "targets = np.zeros((batch_size, 45), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(batch_size):\n",
    "    inputs[i,:] = calibrate_db_12[i][0]\n",
    "    targets[i,calibrate_db_12[i][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed = {\n",
    "    input_12_node:inputs,\n",
    "    target_12_node:targets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:0 Loss:3.52852\n",
      "Epochs:9 Loss:1.5357\n",
      "Epochs:19 Loss:0.006028\n",
      "Epochs:29 Loss:6.20592e-05\n",
      "Epochs:39 Loss:4.56565e-06\n",
      "Epochs:49 Loss:5.18552e-06\n",
      "Epochs:59 Loss:4.01731e-06\n",
      "Epochs:69 Loss:2.55106e-06\n",
      "Epochs:79 Loss:1.63316e-06\n",
      "Epochs:89 Loss:1.14441e-06\n",
      "Epochs:99 Loss:8.70226e-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    if (i + 1) % 10 == 0 or i == 0:\n",
    "        cost = loss.eval(feed)\n",
    "        print('Epochs:{}'.format(str(i)), 'Loss:{}'.format(str(cost)))\n",
    "    train_step.run(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:0 Loss:3.80697\n",
      "Epochs:9 Loss:2.33776\n",
      "Epochs:19 Loss:2.30832\n",
      "Epochs:29 Loss:2.30634\n",
      "Epochs:39 Loss:2.30348\n",
      "Epochs:49 Loss:2.3029\n",
      "Epochs:59 Loss:2.30273\n",
      "Epochs:69 Loss:2.30267\n",
      "Epochs:79 Loss:2.30259\n",
      "Epochs:89 Loss:2.30259\n",
      "Epochs:99 Loss:2.30259\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    if (i + 1) % 10 == 0 or i == 0:\n",
    "        cost = loss.eval(feed)\n",
    "        print('Epochs:{}'.format(str(i)), 'Loss:{}'.format(str(cost)))\n",
    "    train_step.run(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
