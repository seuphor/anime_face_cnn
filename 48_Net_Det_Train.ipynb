{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models._48_net import detect_48Net\n",
    "from models._24_net import calib_24Net\n",
    "from utils.params import *\n",
    "from utils.sampling import GetSample\n",
    "from utils import save\n",
    "from utils.utilities import *\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples Checked..\n",
      "IMG_INDEX:112/112\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "get = GetSample()\n",
    "pos_db_48 = get.get_pos_img(p_net_48)\n",
    "save.saver('pos_db_48.txt', pos_db_48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from D:\\programing project\\python project\\trying stuffpickle/\n",
      "Loaded from D:\\programing project\\python project\\trying stuffpickle/\n",
      "Loaded from D:\\programing project\\python project\\trying stuffpickle/\n"
     ]
    }
   ],
   "source": [
    "inputs = np.zeros((p_batch_size ,p_net_48, p_net_48, 3))\n",
    "targets = np.zeros((p_batch_size, 1))\n",
    "pos_db_48 = save.loader('pos_db_48.txt')\n",
    "neg_db_48 = save.loader('neg_db_48_filter.txt')\n",
    "valid_db_48 = save.loader('valid_db_48.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_input = np.array([arr[0] for arr in valid_db_48])\n",
    "valid_target = np.array([arr[1] for arr in valid_db_48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detnet_evaluation(pred_arr, true_arr, thres=.5):\n",
    "    pred_idx = np.where(pred_arr>thres)[0]\n",
    "    pred_tf = (pred_arr>thres)*1\n",
    "    #print(len(pred_idx))\n",
    "    correct_idx = len(np.where(true_arr[pred_idx] == 1)[0]) # Predicted Positives that are true positive\n",
    "    true_len = np.sum(true_arr == 1.0)\n",
    "    \n",
    "    correct_numb = np.sum((pred_tf==true_arr))\n",
    "    return correct_idx / true_len, correct_idx/(len(pred_idx)+1e-7)\n",
    "\n",
    "def f1_score(recall, precision, weight=1.0):\n",
    "    a = weight**2\n",
    "    return (a + 1)*recall*precision / a*(recall + precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(valid_target == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias = [0.0]\n",
    "lr = [1e-3, 5e-4, 1e-4, 5e-5]\n",
    "reg = [0.0, 1e-3]\n",
    "epochs = 500\n",
    "neg_db_48 = neg_db_48[:10080,:]\n",
    "batch_len = int(len(neg_db_48) / p_neg_batch)\n",
    "total_iters = batch_len * epochs\n",
    "\n",
    "pickle_path = 'saver/det_48/mean_var/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Bias_init:0.0   Learning_rate:0.001   Regularization:0.0**\n",
      "Iteration: 122/17500    Loss: 0.01141/0.0733929    Bad: 0    R/AC/F1: 41.666%/86.206%/91.863    MAX_R/AC/F1: 41.666%/86.206%/91.863   Max_Epo: 3"
     ]
    }
   ],
   "source": [
    "for r in reg:\n",
    "    for b in bias:\n",
    "        for l in lr:\n",
    "            record_l = l\n",
    "            print('\\n**Bias_init:{}   Learning_rate:{}   Regularization:{}**'.format(b,l,r))\n",
    "            \n",
    "            # initialize Batch selection function\n",
    "            batch_sel_pos = batch_selection()\n",
    "            batch_sel_neg = batch_selection()\n",
    "            \n",
    "            bad_cnt = 0\n",
    "            max_epo = 0\n",
    "            max_true_acc = 0\n",
    "            max_recall = 0\n",
    "            max_f1 = 0\n",
    "            \n",
    "            # Build the 48_det_CNN\n",
    "            g = tf.Graph()\n",
    "            with g.as_default():\n",
    "                input_48_node = tf.placeholder(tf.float32, [None,None,None,3])\n",
    "                target_48_node = tf.placeholder(tf.float32, [None,1])\n",
    "                keep_prob = tf.placeholder(tf.float32)\n",
    "                det_net_48 = detect_48Net(input_48_node, target_48_node, keep_prob=keep_prob, lr=l, bias_init=b, reg=r)\n",
    "                \n",
    "                sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "                  allow_soft_placement=True, log_device_placement=True), graph=g)\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            total_mean_var_list = np.zeros((4,64))\n",
    "            offset_scale_arr = np.zeros((4,64))\n",
    "            \n",
    "            pred_arr = det_net_48.h_fc_conv2_flat.eval(feed_dict={input_48_node:valid_input, keep_prob:1.0})\n",
    "            recall, true_acc = detnet_evaluation(pred_arr, valid_target, .5)\n",
    "            f1 = f1_score(recall, true_acc)\n",
    "            \n",
    "            for epo in range(epochs):\n",
    "                loss = 0\n",
    "                \n",
    "                if (epo+1) % 2 == 0:\n",
    "                    pred_arr = det_net_48.h_fc_conv2_flat.eval(feed_dict={input_48_node:valid_input, keep_prob:1.0})\n",
    "                    recall, true_acc = detnet_evaluation(pred_arr, valid_target, .5)\n",
    "                    f1 = f1_score(recall, true_acc)\n",
    "                    \n",
    "                    if f1 >= max_f1:\n",
    "\n",
    "                        max_true_acc = true_acc\n",
    "                        max_recall = recall\n",
    "                        max_f1 = f1\n",
    "\n",
    "                        max_epo = epo\n",
    "                        bad_cnt = 0\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, p_model_dir + 'det_48/max/_Net48_b={}_lr={}_reg={}.ckpt'.format(b,record_l,r))\n",
    "                        \n",
    "                        with open(pickle_path + '_Net48_MV_b={}_lr={}_reg={}.txt'.format(b, record_l, r), 'wb') as f:\n",
    "                            pickle.dump(mean_var_list, f)\n",
    "                    else:\n",
    "                        bad_cnt += 1\n",
    "                        \n",
    "                if bad_cnt % 4 == 0:\n",
    "                    l = l * .98\n",
    "                    \n",
    "                if bad_cnt > 50:\n",
    "                    break\n",
    "                \n",
    "                mean_var_list = np.zeros((4,64))\n",
    "                \n",
    "                \n",
    "                for i in range(batch_len):\n",
    "                    cur_iter = i + batch_len * epo\n",
    "                    \n",
    "                    inputs[:p_pos_batch,:] = batch_sel_pos.next_batch_single(pos_db_48, p_pos_batch)\n",
    "                    targets[:p_pos_batch,:] = np.ones((p_pos_batch, 1))\n",
    "                    inputs[p_pos_batch:,:] = batch_sel_neg.next_batch_single(neg_db_48, p_neg_batch)\n",
    "                    targets[p_pos_batch:,:] = np.zeros((p_neg_batch, 1))\n",
    "\n",
    "                    feed = {\n",
    "                        input_48_node:inputs,\n",
    "                        target_48_node:targets,\n",
    "                        keep_prob: 1.0\n",
    "                    }\n",
    "                    \n",
    "                    feed_train = {\n",
    "                        input_48_node:inputs,\n",
    "                        target_48_node:targets,\n",
    "                        keep_prob: .5\n",
    "                    }\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    loss += det_net_48.loss.eval(feed)\n",
    "                    single_loss = det_net_48.loss.eval(feed)\n",
    "\n",
    "                    det_net_48.train_step.run(feed_train)\n",
    "                    \n",
    "                    mean_var_list[0,:] += det_net_48.mean_wconv1.eval(feed)\n",
    "                    mean_var_list[1,:] += det_net_48.var_wconv1.eval(feed)\n",
    "                    mean_var_list[2,:] += det_net_48.mean_wconv2.eval(feed)\n",
    "                    mean_var_list[3,:] += det_net_48.var_wconv2.eval(feed)\n",
    "\n",
    "                    sys.stdout.write('\\rIteration: ' + str(cur_iter+1) + '/' + str(total_iters) + \\\n",
    "                                     '    Loss: ' + str(loss/(cur_iter+1))[:7] + '/' + str(single_loss) + \\\n",
    "                                     '    Bad: ' + str(bad_cnt) + \\\n",
    "                                     '    R/AC/F1: ' + str(recall*100)[:6] + '%' + '/' + str(true_acc*100)[:6] + '%/' + \\\n",
    "                                     str(f1*100)[:6] + \\\n",
    "                                     '    MAX_R/AC/F1: ' + str(max_recall*100)[:6] + '%' + '/' + str(max_true_acc*100)[:6] + '%/' + \\\n",
    "                                     str(max_f1*100)[:6] + \\\n",
    "                                     '   Max_Epo: ' + str(max_epo))\n",
    "                mean_var_list = mean_var_list / batch_len\n",
    "            \n",
    "            #offset_scale_arr[0,:] = det_net_48.of_conv1.eval()\n",
    "            #offset_scale_arr[1,:] = det_net_48.sc_conv1.eval()\n",
    "            #offset_scale_arr[2,:] = det_net_48.of_conv2.eval()\n",
    "            #offset_scale_arr[3,:] = det_net_48.sc_conv2.eval()\n",
    "            \n",
    "            #total_mean_var_list = total_mean_var_list / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_sel_pos = batch_selection()\n",
    "batch_sel_neg = batch_selection()\n",
    "tmean_var_list = np.zeros((4,64))\n",
    "for i in range(batch_len):\n",
    "    inputs[:p_pos_batch,:] = batch_sel_pos.next_batch_single(pos_db_48, p_pos_batch)\n",
    "    targets[:p_pos_batch,:] = np.ones((p_pos_batch, 1))\n",
    "    inputs[p_pos_batch:,:] = batch_sel_neg.next_batch_single(neg_db_48, p_neg_batch)\n",
    "    targets[p_pos_batch:,:] = np.zeros((p_neg_batch, 1))\n",
    "    feed = {\n",
    "            input_48_node:inputs,\n",
    "            target_48_node:targets,\n",
    "            keep_prob: 1.0\n",
    "        }\n",
    "    tmean_var_list[0,:] += det_net_48.mean_wconv1.eval(feed)\n",
    "    tmean_var_list[1,:] += det_net_48.var_wconv1.eval(feed)\n",
    "    tmean_var_list[2,:] += det_net_48.mean_wconv2.eval(feed)\n",
    "    tmean_var_list[3,:] += det_net_48.var_wconv2.eval(feed)\n",
    "    \n",
    "tmean_var_list = tmean_var_list / batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.52908495e-01,   4.61446481e-01,   9.81779911e-01,\n",
       "         7.76999937e-01,   8.37855125e-03,   1.85452061e-01,\n",
       "         4.95153850e-01,   3.40492738e-01,   3.32354470e-03,\n",
       "         1.45554514e-02,   1.93674001e-01,   1.45450069e-01,\n",
       "         1.60463558e+00,   4.32460479e-01,   2.84323978e-01,\n",
       "         1.82362037e-01,   2.59806626e-02,   1.41569671e-01,\n",
       "         2.36387303e-01,   2.73599435e-01,   1.29298050e-02,\n",
       "         4.42843865e-02,   2.44863621e-02,   1.81766756e-02,\n",
       "         8.85730882e-02,   1.30457179e-02,   2.77885529e-01,\n",
       "         1.72764273e+00,   2.16383407e-01,   1.62746808e-01,\n",
       "         1.09626971e+00,   1.08306128e-01,   3.91063710e-01,\n",
       "         1.36166664e+00,   7.54949956e-01,   1.46540683e-01,\n",
       "         1.02859631e-03,   1.83600186e-01,   1.24208521e-03,\n",
       "         1.34620617e-01,   1.62193016e-02,   7.74461877e-01,\n",
       "         1.12280480e+00,   1.97339045e-01,   1.40744009e-03,\n",
       "         5.69657329e-02,   2.31870877e-01,   5.67934128e-02,\n",
       "         8.20592523e-02,   1.25003482e-02,   5.60603821e-01,\n",
       "         1.18031788e+00,   2.24558753e-02,   2.81907625e-02,\n",
       "         5.75410229e-02,   1.32695860e-01,   1.71915858e-01,\n",
       "         9.48637417e-01,   3.90084069e-02,   2.78511034e-01,\n",
       "         1.85664753e-01,   1.34089427e+00,   3.42748911e-02,\n",
       "         2.13978020e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmean_var_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.50960941e-01,   4.63285848e-01,   9.82836609e-01,\n",
       "         7.77118596e-01,   8.37059833e-03,   1.85686649e-01,\n",
       "         4.93709628e-01,   3.37909466e-01,   3.25630203e-03,\n",
       "         1.44065173e-02,   1.93330073e-01,   1.43831174e-01,\n",
       "         1.60672680e+00,   4.33834830e-01,   2.88563408e-01,\n",
       "         1.83027148e-01,   2.59973982e-02,   1.41912745e-01,\n",
       "         2.36671602e-01,   2.75211393e-01,   1.28892390e-02,\n",
       "         4.38240043e-02,   2.45335301e-02,   1.82645093e-02,\n",
       "         8.72009531e-02,   1.29934340e-02,   2.78458254e-01,\n",
       "         1.72943207e+00,   2.15405279e-01,   1.58615994e-01,\n",
       "         1.09639059e+00,   1.07906934e-01,   3.96223021e-01,\n",
       "         1.36200685e+00,   7.56211868e-01,   1.45128296e-01,\n",
       "         1.02876989e-03,   1.81334391e-01,   1.24890130e-03,\n",
       "         1.31621446e-01,   1.61902729e-02,   7.76542032e-01,\n",
       "         1.12573348e+00,   1.95331793e-01,   1.39888260e-03,\n",
       "         5.64789913e-02,   2.31226784e-01,   5.67178888e-02,\n",
       "         8.27340177e-02,   1.29700552e-02,   5.65610114e-01,\n",
       "         1.18048398e+00,   2.23518440e-02,   2.84441540e-02,\n",
       "         5.70257812e-02,   1.32948223e-01,   1.71152407e-01,\n",
       "         9.48793175e-01,   3.90405210e-02,   2.75225153e-01,\n",
       "         1.84045549e-01,   1.34362003e+00,   3.42820880e-02,\n",
       "         2.12794933e-01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_var_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    input_48_node = tf.placeholder(tf.float32, [None,None,None,3])\n",
    "    target_48_node = tf.placeholder(tf.float32, [None,1])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    det_net_48 = detect_48Net(input_48_node, target_48_node, keep_prob=keep_prob, lr=l, bias_init=b, reg=r)\n",
    "\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True), graph=g)\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed={\n",
    "    input_48_node:np.zeros((1,48,48,3))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_net_48.mean_wconv1.eval(feed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tf.Variable(tf.random_normal([128, 32, 32, 64]))\n",
    "axis = list(range(len(img.get_shape()) - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
